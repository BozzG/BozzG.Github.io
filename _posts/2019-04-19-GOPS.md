---
layout: post
title: 深圳GOPS会议参会笔记
date: 2019-04-14
tags: 随笔
---

## GOPS 参会记录

公司让我参加深圳的GOPS<br>重点关注：AIOPS，监控告警建设，故障管理及基础架构<br>含金量：华为、去哪儿网、头条的干货较多，其余水分和广告成分较大。

#### AIOPS：

​    总结：业界对AIOPS的讨论回归理性，相比去年年底的Gdevops会议“跑步进入AIOPS”的气氛，这次会议几个大厂（华为、去哪儿、美团）展示出来的态度都是先做好DataOPS，尝试用AI处理DataOPS中遇到的问题。因此分享的重点多落在监控数据平台部分，关于其中AI的落地，华为云有数据智能的概念，里面有比较好的一套人员职能划分，主要解决的场景包括：异常检测、根因分析、故障预测、舆情分析；去哪儿网更多分享了AI用于预测方面的思路，以及将工业中较为成熟的PHM理论迁移到互联网中。其余小厂关于AIOPS的分享都只是在摆弄概念，无实质成果。

##### 华为：

人员分工：<br>AIOPS中，通常需要包括：算法模型工程师（AI）、运维数据开发工程师（data），运维团队<br>数据工程师	：整理数据输送给AI工程师AI工程师	：建立模型，调参，通过AB测试，判定模型优劣，决定实际模型<br>运维团队	：负责标注数据，需要学习如何使用模型进行检测/预测

落地场景：

+ 异常检测：介绍了一些数据预处理方法，总体而言并无太大亮点，也是和裴丹教授下面的学生合作，与之前南开大学的分享大同小异，并且最后承认了在目前阶段由于深度学习具有不可解释性等原因，传统的统计方法，可能更适合。
+ 根因分析：试图解决故障发生后，人工定位时间长效率低的问题。主要事先需要建立调用链元数据，配合人工设置的规则，建立根因分析模型（我的理解就是类似故障树的存在），故障发生后，根据故障特征通过皮尔逊、余弦夹角算法定位异常模块（我理解其实就是根据依赖关系和强弱通过故障树这类模型，找出问题模块）。例子给出的MDRCA模型，就是根据贡献度（依赖强弱）和异常程度，找出根因。
+ 故障预测：基本都能想到的AI在OPS中的切入点，从被动解决转为主动防御。落地方案和我们在做的基本一致，都是通过离线训练和在线匹配，算法也是通过LSTM。区别是他们通过模板化技术，将这个预测能力泛化到各种格式的数据，包括日志数据，这点我们的预测目前必须是指标这种单维度数值型。
+ 舆情分析：分析反馈文本，拆分多级关键字，观察各级关键字出现的数量增减趋势。但需要设置关键字级别，可以参考自然语言处理的方法，提取关键字。我认为这个分析应该能用到日志分析上。比如收集ERROR日记，分析统计程序报错原因，反馈优化。   

##### 去哪儿网

预测：<br>容量预测：	说白了就是对磁盘空间、CPU、链接数这些量化数据进行预测。<br>故障预测：	在健康管理系统的基础上，根据对健康度进行预测。

PHM-健康管理系统（我研究生阶段进行过航电健康管理系统的相关研究，有必要可以讨论）：
简单说健康管理系统，就是根据对象自身的属性参数，环境参数，估算对象的可靠性和可用度。传统工业PHM的对象可以简单的单个机件，测量的参数包括自身的密度，受力负载，环境参数温度、湿度等，也可以是整个系统如雷达系统。
健康度说白了就是一套评分机制，通过各项参数进行计算得到一个数值。另外常见的会估算出，对象的剩余寿命，或对象某项能力的残余值，如一台损坏雷达的可靠测量范围。

我对于PHM在互联网中落地的猜想

1. 以服务为对象进行健康管理。服务，特别是无状态服务原则上只是一个虚拟体，自身参数不会随时间累积而发生变化，有状态服务需要考虑服务所记录的状态。应该更多的关注服务所在机器的机器状态，对应传统PHM中的环境参数。注意服务调用成功率是作为“果”出现的，用结果去评估健康度并不合理。
2.  以基础设施为对象进行健康管理，比较贴近工业中的PHM理论。自变量可以考虑机器中服务个数，和各自对资源的占用情况。

#### 监控数据平台：

总结：监控数据的讨论，主要围绕海量数据存储/查询，和数据预处理/聚合。对比了其他厂商，我们目前做的还不错。可以上升的空间有：存储能力还有上升空间。容量方面，目前存3个月数据已勉强。可靠性方面，副本模式好像一直没用。通道能力，Kafka的消费能力。

1. 华为云也采用有kafka + clickhouse，数据量高达400T/天，但并没有给出更具体的方案。
2. 通过优化数据预处理算法压缩数据量。
3. 培训运维人员使用数据的能力（配图、标注、使用AI模型）。
4. 监控平台自己对于自身的监控，可靠性报障。

#### 告警有效性：

现实困境：	

1. 滥设告警：不知道应该设置什么告警于是什么都报无效告警多
2. 什么都报 = 什么都没报
3. 告警规则不更新：业务发展+技术迭代后没有及时更新告警规则
4. 告警接收人不清楚告警来源：人员交替没有做好告警交接工作
5. 对告警-BUG之间的关系模糊：监控&告警并不能替代QA与责任心
6. 观念错误：一键傻瓜式的告警设置的不存在的

解决：

1. 关联Appcode：明确告警来源，明确告警接收人，明确告警管理者<br>（比通过业务ID设置告警更加严格，为每一个服务设置全局唯一的Appcode，管理Appcode和人的对应关系）
2. 监控长时间没有结束的告警：调整告警设置 or 人员培训<br>（一方面是针对告警设置不合理，如阈值不合理，导致告警频发的情况，另一方面针对接收人员不处理告警的情况)
3. 提供各种告警设置方法：单指标、多指标聚合、同比/环比、函数、组合
4. 教育&培训

想法：

1. 设置故障级别告警，通过运维机器人直接拉群解决。

#### 故障管理：

1. 管理故障处理和整改的进度（已经完成）
2. 历史故障数据的利用：
   1. 故障演练（已在推动）
   2. 收集故障期间的相关的告警信息，分析告警和故障之间的关联关系，建立故障知识库。（未听说有相关计划）

 #### 总结：

1. 重视数据价值，先扎实做好DataOps，尝试用AI解决问题。
2. AI技术可以先考虑在离线或事后的场景下实践。
3. 重视技术运营。举个例子，指标的元数据管理是我们负责，指标字段大量冗余也是已知很久的问题。但据我所知是别的组的同学推动在做指标聚合的事情，预计能把相关指标的数据量压缩到十分之一。我认为，我们把工具做出来了，如何引导用户更好使用这个工具从到达到更优的效果，也是我们的责任，或者说是达到目标的一个更简便的一条路。
